import os
import re
import json
from typing import List, Generator
from langchain_core.messages import (
    ToolMessage
)
from langchain_core.tools import tool
from DatabaseManager import DatabaseManager
from VectorStore import VectorStore
from langchain_ollama import ChatOllama
from utils import convert_prompt_to_langchain_messages, get_specific_civil_code_article as get_article
from dotenv import load_dotenv

load_dotenv("../.env")

user_messages = []  # Liste pour stocker les messages de l'utilisateur # Pas utilis√© pour actuellement

MAX_TOKENS = 4096  # Nombre maximum de tokens pour le mod√®le
MAX_ITERATIONS = 3  # Nombre maximum d'it√©rations pour la conversation

db_manager = DatabaseManager(os.getenv("EMBEDDED_MODEL"))
vectorstore = VectorStore(db_manager=db_manager)
llm = ChatOllama(
            model=os.getenv("OLLAMA_MODEL"),
            temperature=0.7,
            num_ctx=MAX_TOKENS,
            reasoning=False,
        )



@tool
def get_specific_civil_code_article(article_number: str) -> str:
    """
    R√©cup√®re un article sp√©cifique du code civil fran√ßais √† partir de son num√©ro.
        param article_number: Le num√©ro de l'article √† r√©cup√©rer 
        return: Le texte complet de l'article du code civil
    """

    print(f"üîç Recherche de l'article {article_number} dans le code civil...")

    return get_article(article_number)


# Pas utilis√© actuellement
@tool
def get_previous_user_message() -> List[str]:
    """
    A utiliser lorsque tu ne comprends pas la question de l'utilisateur pour voir ce qu'il a dit pr√©c√©demment.
    R√©cup√®re le dernier message de l'utilisateur.
        return: Dernier message de l'utilisateur
    """
    global user_messages

    if not user_messages:
        return "Aucun message utilisateur pr√©c√©dent disponible."
    return [message for message in user_messages]


@tool
def get_context_on_french_civil_code(query: str) -> str:
    """
    Permet de faire une recherche √† l'int√©rieur du code civil fran√ßais.
        param query: La requ√™te utilisateur
        return: Contexte issus du code civil fran√ßais
    """

    context = vectorstore.get_context(query)
    print(f"Contexte r√©cup√©r√© pour la requ√™te '{query}': {context[:100]}...")  # Affiche les 100 premiers caract√®res du contexte
    return context



    

class OllamaAgent:
    def __init__(self):
        self._max_iterations = MAX_ITERATIONS
        
        # Initialiser le LLM avec des param√®tres optimis√©s
        self._llm = llm

    def process_message(self, messages: list[dict[str,str]]) -> Generator[str, None, None]:
        global user_messages

        system_message = {
            "role": "system",
            "content": """   
            Vous √™tes un assistant IA capable d'avoir une conversation et d'utiliser plusieurs outils pour r√©pondre aux questions des utilisateurs.
            Votre r√©ponse doit √™tre bas√©e UNIQUEMENT sur les informations obtenues via ces outils et JAMAIS √† partir de votre propre connaissance.

            R√àGLES FONDAMENTALES :
            1. ANALYSEZ d'abord la question pour d√©terminer quels outils sont n√©cessaires
            2. UTILISEZ les outils appropri√©s pour collecter les informations requises
            3. COMBINEZ les r√©sultats de tous les outils utilis√©s pour formuler une r√©ponse compl√®te
            4. R√âPONDEZ UNIQUEMENT bas√© sur les informations obtenues via les outils
            5. CITEZ toujours les sources (articles du code civil, r√©sultats d'op√©rations, etc.) dans votre r√©ponse
            6. R√âPONDEZ de mani√®re naturelle et conversationnelle, ne mentionnez JAMAIS les outils que vous utilisez
            
            PROCESSUS DE TRAVAIL :
            - Identifiez les informations manquantes pour r√©pondre √† la question
            - S√©lectionnez et utilisez les outils pertinents (vous pouvez en utiliser plusieurs)
            - Attendez les r√©sultats de tous les outils avant de r√©pondre
            - Synth√©tisez les informations collect√©es en une r√©ponse coh√©rente et structur√©e
            
            PR√âSENTATION DES R√âPONSES :
            - Structurez votre r√©ponse de mani√®re claire avec des sections si n√©cessaire
            - Indiquez les sources des informations
            - Si les outils ne fournissent pas assez d'informations, demandez des pr√©cisions
            - R√©pondez comme si vous aviez naturellement acc√®s √† ces informations
            
            N'inventez jamais d'informations. Utilisez exclusivement les donn√©es obtenues via les outils disponibles.
            Ne faites jamais r√©f√©rence aux "outils", "recherches" ou "bases de donn√©es" dans vos r√©ponses.
            """
        }

        processed_messages = []
        user_messages = [] # R√©initialiser la liste des messages de l'utilisateur

        # Limiter le nombre de messages √† 10 derniers messages pour √©viter les surcharges
        for message_num in range(len(messages)-1, len(messages) - 11, -1):
            if message_num < 0:
                break
            else :
                processed_messages.insert(0, messages[message_num])

        # R√©cup√©rer tous les messages pr√©c√©dents de l'utilisateur
        for message in reversed(messages):
            if message["role"] == "user":
                user_messages.append(message["content"])


        # Cr√©er un agent React avec les outils n√©cessaires
        agent = self._llm.bind_tools([get_context_on_french_civil_code, get_specific_civil_code_article])
        processed_messages.insert(0, system_message)
        processed_messages = convert_prompt_to_langchain_messages(processed_messages)

        for i in range(1, self._max_iterations+1):
            # Appel au LLM
            response = agent.invoke(processed_messages)

            # V√©rifier s'il y a des appels d'outils
            if response.tool_calls and i < self._max_iterations:
                
                
                # Traiter tous les appels d'outils
                tools_results = []
                for tool_call in response.tool_calls:
                    if tool_call["name"] == "get_context_on_french_civil_code":
                        print("üîß Utilisation de l'outil de r√©cup√©ration de contexte")
                        content = str(get_context_on_french_civil_code.invoke(tool_call["args"]))
                        tools_results.append(ToolMessage(content=content, tool_call_id=tool_call["id"]))
                    elif tool_call["name"] == "get_previous_user_message":
                        print("üîß Utilisation de l'outil de r√©cup√©ration du message utilisateur pr√©c√©dent")
                        content = str(get_previous_user_message.invoke(tool_call["args"]))
                        tools_results.append(ToolMessage(content=content, tool_call_id=tool_call["id"]))
                    elif tool_call["name"] == "get_specific_civil_code_article":
                        print("üîß Utilisation de l'outil de r√©cup√©ration d'un article sp√©cifique du code civil")
                        content = str(get_specific_civil_code_article.invoke(tool_call["args"]))
                        tools_results.append(ToolMessage(content=content, tool_call_id=tool_call["id"]))

                # Ajouter les r√©sultats des outils aux processed_messages
                processed_messages.extend(tools_results)
                continue  # Retourner au d√©but de la boucle pour traiter la r√©ponse suivante
            
            else:
                # Pas d'appels d'outils, streamer la r√©ponse finale
                for chunk in agent.stream(processed_messages):
                    yield chunk.content

                break  # Sortir de la boucle apr√®s avoir trait√© la r√©ponse finale


    

if __name__ == "__main__":

    agent = OllamaAgent("qwen3:4b")
    #"llama3.2:3b-instruct-q5_1"

    message = {"role": "user", "content": " 4 * 7 = ?."}
    agent.process_message(message)
    
    